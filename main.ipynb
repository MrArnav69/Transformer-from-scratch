{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S73ojjpKZuG"
      },
      "source": [
        "# Building a Transformer from Scratch\n",
        "\n",
        "This notebook will lead you through building your own transformer from the ground up. We'll go through positional encoding, multi-headed attention, and more. Then we'll address a simple next-token-prediction problem where we try to predict the next number in a cyclic sequence (0->1->2->3...->9->0).\n",
        "\n",
        "Note: At the bottom of some of the coding cells there are notes about useful methods, helpful syntax, or some hints that can make the code cell easier and more clean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_mL2f8ZLKDq"
      },
      "source": [
        "## Why are transformers important?\n",
        "\n",
        "From lecture we learned that transformers make use of a concept called **attention** to keep track of the importance of a token in a given sequence to every other token in that sequence.\n",
        "\n",
        "Tokens are often modeled as words with sequences being sentences or paragraphs, but can also be extended to other applications. We briefly touched upon things like ViTs (Vision Transformers) that model tokens as pixels and sequences as images, and audio transformers that model tokens as audio signals and secions of audio data and sequences as a stream of audio.\n",
        "\n",
        "In the task we're addressing token we can consider our tokens to be single numbers and our sequence to be the next number to follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dNNk9Do2KHFf"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5fm1XpqL_OG"
      },
      "source": [
        "As you may recall from lecture, transformers use positional encoding to keep track of the order of words so they know which words came first.\n",
        "\n",
        "Since transformers process input tokens in parallel rather than sequentially, they lack an inherent sense of word order. To address this, positional encodings are added to the input embeddings, allowing the model to capture the relative and absolute positions of tokens.\n",
        "\n",
        "There are two common types of positional encoding: sinusoidal and learned. The original transformer paper used sinusoidal functions of different frequencies to generate fixed position vectors, enabling the model to extrapolate to longer sequences. Alternatively, learned positional embeddings treat positions like tokens and learn their representations during training. In both cases, these encodings are added to the token embeddings before entering the self-attention layers, enabling the model to reason about position-dependent meaning.\n",
        "\n",
        "\\\n",
        "\n",
        "Today we'll be using **sinusoidal encodings**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M_rDnjCxKKkM"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len: int, dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Explainations Below\n",
        "        pos_enc = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2, dtype=torch.float32) * (-math.log(10000.0) / dim))\n",
        "\n",
        "        # TODO\n",
        "        # Set every EVEN column (0, 2, 4...) in pos_enc to sin(position * div_term)\n",
        "        # Set every ODD columns (1, 3, 5...) in pos_enc to cos(position * div_term)\n",
        "        # Make sure to use the right indexing for position (e.g. position[0] (or just 0) for column 0)\n",
        "        # or apply tensor math to calculate all values in one line\n",
        "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Using register_buffer instead of just self.pos_enc = ... means this will not be treated as a trained parameters\n",
        "        # This is intended for pos_enc because we are using set sinusoidal encodings so we don't want them to change\n",
        "        self.register_buffer(\"pos_enc\", pos_enc.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        return x + self.pos_enc[:, : x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U313JuZhYSgO"
      },
      "source": [
        "**Variable Descriptions**\n",
        "\n",
        "`pos_enc`: A matrix holding the positional encodings for each word.\n",
        "\n",
        "`position`: An array with index values for each word up to max_len. ([0, 1, 2..., max_len-1])\n",
        "\n",
        "`div_term`: A scaling factor to avoid repeating sine and cosine frequencies\n",
        "\n",
        "**Useful Functions**\n",
        "\n",
        "`torch.sin`, `torch.cos`\n",
        "\n",
        "**Hints**\n",
        "\n",
        "- You can use the syntax arr[:, 0::2] to set all values in the even columns (start at 0 and step 2 columns at a time until the end)\n",
        "- You can use the syntax `pos_enc[:, 0::2] = torch.sin(position * div_term)` to set the positional encodings for the even columns and modify this code for the odd columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGsN0TIM3Qg"
      },
      "source": [
        "In this cell we'll focus on implementing our Query, Key, and Value weights for our attention block and some of the calculations involved. Since our Query, Key, and Value matrices are trainable weights we can represent them as linear layers and learn their associated weight values.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://drive.google.com/uc?export=view&id=1-J6feRuS3qP9dD79Zq-VdVsL9432WMCF\" width=\"600px\"></center>\n",
        "\n",
        "\n",
        "The graphic above gives some intuition as to the size of the QKV matrices/linear layers and shows the equation we need to implement (highlighted in yellow) which is then used in the final output (in grey). The equation you need to implement is also listed below\n",
        "\n",
        "$$\\frac{Q \\cdot K^\\intercal}{\\sqrt{dim}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7v97QLglKOzV"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, proj_dim: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Double check that the proj_dim is divisible by the number of heads\n",
        "        assert proj_dim % num_heads == 0, \"Hidden dimension (proj_dim) must be divisible by the number of heads (num_heads)\"\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = proj_dim // num_heads\n",
        "\n",
        "        # TODO: Map a linear layer to each input projection\n",
        "        self.q_proj = nn.Linear(proj_dim, proj_dim)\n",
        "        self.k_proj = nn.Linear(proj_dim, proj_dim)\n",
        "        self.v_proj = nn.Linear(proj_dim, proj_dim)\n",
        "        self.o_proj = nn.Linear(proj_dim, proj_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x: torch.Tensor) -> torch.Tensor:  # (B, L, H) -> (B, h, L, d)\n",
        "        B, L, H = x.shape\n",
        "        x = x.view(B, L, self.num_heads, self.d_k)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x: torch.Tensor) -> torch.Tensor:  # (B, h, L, d) -> (B, L, H)\n",
        "        B, h, L, d = x.shape\n",
        "        return x.transpose(1, 2).contiguous().view(B, L, h * d)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
        "\n",
        "        # TODO: Use one of the methods above to split the heads for each matrix\n",
        "        Q = self.split_heads(self.q_proj(x))\n",
        "        K = self.split_heads(self.k_proj(x))\n",
        "        V = self.split_heads(self.v_proj(x))\n",
        "\n",
        "        K_transpose = K.transpose(-2, -1)\n",
        "        # TODO: Calculate the part of the Z matrix highlighted above (the inside of the softmax function)\n",
        "        scores = (Q @ K_transpose) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # The @ sign denotes matrix multiplication in numpy\n",
        "        context = attn @ V  # (B, h, L, d)\n",
        "        context = self.combine_heads(context)\n",
        "\n",
        "        return self.o_proj(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3OJaxfcciq"
      },
      "source": [
        "**Useful Syntax**\n",
        "\n",
        "`@` in PyTorch and number is used for matrix multiplication (e.g. Q @ K multiplies matrix Q by matrix K)\n",
        "\n",
        "**Hints**\n",
        "\n",
        "- In matrix multiplication the communtative property does NOT hold, so the ORDER MATTERS.\n",
        "\n",
        "- All of the projection matrices (e.g. self.q_proj) should be initialized as the save value.\n",
        "\n",
        "- When calling split_heads perform a forward pass through the current projections (e.g. Q = split_heads(self.q_proj(x)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVOnL0nmDT38"
      },
      "source": [
        "In the following code block we'll set up a positionwise feed forward block using pre-existing torch layers and a ReLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-H_CtGn2KQMv"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, ff_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # This probably doesn't need to be a TODO\n",
        "        # TODO: Fill in the layers with appropriate parameters\n",
        "        self.fc1 = nn.Linear(hidden_dim, ff_dim)\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(ff_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        # TODO: Fill out the forward pass\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qglPC4YNOi2U"
      },
      "source": [
        "Next we'll put all of that together along with some layer normalization to create our transformer block. Take a look at the image below to see how our block aligns with the transformer architecture.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://d1.awsstatic.com/GENAI-1.151ded5440b4c997bac0642ec669a00acff2cca1.png\" width=\"300px\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3zlbCEljKRoL"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, proj_dim: int, num_heads: int, ff_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # TODO: Fill out our main transformer block according to the architecture above and the layers we've made\n",
        "        self.multihead_attn = MultiHeadSelfAttention(proj_dim, num_heads, dropout)\n",
        "        self.ln1 = nn.LayerNorm(proj_dim)\n",
        "        self.pos_wise_ff = PositionwiseFeedForward(proj_dim, ff_dim, dropout)\n",
        "        self.ln2 = nn.LayerNorm(proj_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
        "        x = x + self.dropout(self.multihead_attn(x, mask))\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.dropout(self.pos_wise_ff(x))\n",
        "        x = self.ln2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJ1C37-EQbb"
      },
      "source": [
        "Next we'll combine our transformer block from above with our positional encoding layer from earlier to create our full transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJdgIJ0FKUi3"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        hidden_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        vocab_size: int,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Simple one‑hot → hidden projection (equivalent to an embedding matrix)\n",
        "\n",
        "        # TODO: Follow the Transformer Encoder architecture above to implement this block using the layers we have created\n",
        "        self.token_emb = nn.Linear(vocab_size, hidden_dim, bias=False)\n",
        "        self.pos_enc = PositionalEncoding(max_len, hidden_dim)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(hidden_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.ln_final = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x_onehot: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:  # type: ignore[override]\n",
        "        x = self.token_emb(x_onehot)  # (B, L, H)\n",
        "        x = self.pos_enc(x)\n",
        "        # TODO: Replace this if not using nn.ModuleList\n",
        "        for layer in self.transformer_blocks:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return self.ln_final(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CddflayiEwIP"
      },
      "source": [
        "**Useful Functions**\n",
        "\n",
        "`nn.ModuleList()` takes a list of layers as input so we can change the number of transformer blocks we're using using the num_layers parameter. Feel free as well to hardcode to number of transformer blocks and ignore the num_layers parameter.\n",
        "\n",
        "If you opt not to use `nn.ModuleList()` you will have to update the forward pass slightly to match your setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SyEmsw_B_jBZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  20 | loss = 0.0756\n",
            "epoch  40 | loss = 0.0108\n",
            "epoch  60 | loss = 0.0054\n",
            "epoch  80 | loss = 0.0038\n",
            "epoch 100 | loss = 0.0030\n",
            "epoch 120 | loss = 0.0025\n",
            "epoch 140 | loss = 0.0021\n",
            "epoch 160 | loss = 0.0018\n",
            "epoch 180 | loss = 0.0015\n",
            "epoch 200 | loss = 0.0013\n",
            "Input : 8640358829\n",
            "Output: 9751469930\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Synthetic digit dataset\n",
        "vocab = [str(d) for d in range(10)]\n",
        "V = len(vocab)\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "def generate_batch(batch_size: int, seq_len: int):\n",
        "    x_idx = torch.randint(0, V, (batch_size, seq_len))\n",
        "    y_idx = (x_idx + 1) % V  # target is next digit\n",
        "    x_onehot = F.one_hot(x_idx, num_classes=V).float()\n",
        "    return x_onehot, y_idx\n",
        "\n",
        "# Model, head, loss, optimiser\n",
        "model = TransformerEncoder(num_layers=2, hidden_dim=64, num_heads=8, ff_dim=128, vocab_size=V, max_len=50)\n",
        "lm_head = nn.Linear(64, V)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(lm_head.parameters()), lr=3e-3)\n",
        "\n",
        "\n",
        "# TODO: Set model to train mode\n",
        "model.train()\n",
        "\n",
        "# Training Loop\n",
        "epochs, batch_size, seq_len = 200, 32, 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "    digits, labels = generate_batch(batch_size, seq_len)\n",
        "\n",
        "    # TODO: Write the training loop\n",
        "    optimizer.zero_grad()\n",
        "    logits = lm_head(model(digits))\n",
        "    loss = criterion(logits.view(-1, V), labels.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"epoch {epoch:3d} | loss = {loss.item():.4f}\")\n",
        "\n",
        "# Simple Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_seed, _ = generate_batch(1, 10)\n",
        "    logits = lm_head(model(x_seed))\n",
        "    pred_idx = logits.argmax(dim=-1)[0].tolist()\n",
        "    print(\"Input :\", \"\".join(itos[i] for i in x_seed.argmax(dim=-1)[0].tolist()))\n",
        "    print(\"Output:\", \"\".join(itos[i] for i in pred_idx))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMFvUzC0GWiJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
